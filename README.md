# â­ sPCA Challenge
Repositorio del Challenge de reproducibilidad para anÃ¡lisis de datos a gran escala 2025-1

> ğŸ“„ **Citar el paper**: Elgamal, T., Yabandeh, M., Aboulnaga, A., Mustafa, W., & Hefeeda, M. (2015, May). sPCA: Scalable principal component analysis for big data on distributed platforms. In Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data (pp. 79-91).

> âœï¸ **DescripciÃ³n del paper**: 
El paper sPCA: Scalable Principal Component Analysis for Big Data on Distributed Platforms presenta un algoritmo optimizado para realizar AnÃ¡lisis de Componentes Principales (PCA) en entornos distribuidos, diseÃ±ado para manejar grandes volÃºmenes de datos. Los autores identifican limitaciones en las implementaciones existentes de PCA (como las de Mahout y MLlib) en tÃ©rminos de escalabilidad, precisiÃ³n y generaciÃ³n de datos intermedios.

## ğŸ“‚ Estructura del Proyecto
Para la ejeciciÃ³n del cÃ³digo solo usamos los archivos en `input` y el cÃ³digo `run_spca.py`, el resto de los archivos nos sirvieron para hacer pruebas del funcionamiento de la implementaciÃ³n.
```
/challenge
â”‚
â”œâ”€â”€ README.md             # InformaciÃ³n sobre el proyecto ğŸ“š
â”œâ”€â”€ .gitignore            # Archivos ignorados por Git ğŸš«
â”œâ”€â”€ requirements.txt      # Dependencias del proyecto ğŸ“¦
â”œâ”€â”€ /src                  # Intento de un proyecto modular ğŸ”§
â”œâ”€â”€ /notebooks            # Notebooks para pruebas ğŸ“
â”œâ”€â”€ /input                # Datos de entrada en formato txt ğŸ“Š
â”œâ”€â”€ /PPCA.py              # ImplementaciÃ³n naive del PCA ğŸ”¬
â””â”€â”€ /run_spca.py          # Script principal para ejecutar SPCA en Spark ğŸš€
```

---

## ğŸš€ Correr el CÃ³digo en Local

### 1. Clonar el repositorio
```bash
git clone <URL-del-repositorio>
cd challenge
```

### 2. Instalar las dependencias
Instalar las dependencias necesarias con `pip`:
```bash
pip install -r requirements.txt
```

### 3. Ejecutar el cÃ³digo
Correr el script con el siguiente comando, indicando el archivo de entrada, la dimensiÃ³n de salida, las iteraciones mÃ¡ximas y la carpeta de salida:

```bash
python run_spca.py --input "./input/datos_1.txt" --dim 2 --maxIters 100 --output "./output/datos_1_pca_spark"
```

Donde:

* `--input` âœ¨: **Ruta** al archivo de entrada que contiene los datos que se desean reducir dimensionalmente.
* `--dim` ğŸ¯: NÃºmero de dimensiones de salida que deseas obtener.
* `--maxIters` ğŸ”„: NÃºmero mÃ¡ximo de iteraciones para el algoritmo en caso de que no converja.
* `--output` ğŸ’¾: **Ruta** de la carpeta donde se guardarÃ¡n los componentes principales y los datos con la dimensionalidad reducida.

---

## ğŸ–¥ Correr el CÃ³digo en Cluster con Hadoop y Spark
Para correr el cÃ³digo en hadoop es clave subir los archivos de input a hdfs y pasarle el entorno virtual en un paquete para que los demÃ¡s computadores del cluster puedan usar las librerÃ­as.

### 1. Clonar el repositorio en el cluster
```bash
git clone <URL-del-repositorio>
```

### 2. Crear y activar un entorno virtual
Crear un entorno virtual con `venv` y actÃ­varlo:

```bash
python3 -m venv env
source env/bin/activate  # Para Linux/MacOS
# .\env\Scripts\activate  # Para Windows
```

### 3. Instalar las dependencias
Instalar las dependencias del proyecto:
```bash
pip install -r requirements.txt
```

### 4. Comprimir el entorno virtual
Empaquetar el entorno virtual en un archivo `env.tar.gz`:

```bash
tar -czvf env.tar.gz env/
```

### 5. ConfiguraciÃ³n de Spark en `run_spca.py`
Descomentar y configurar las siguientes lÃ­neas en `run_spca.py` para que use el entorno virtual comprimido en el cluster:

```python
############# Start Spark Session #############################

# For client mode
# spark = SparkSession.builder.appName("sPCA").getOrCreate()

# For cluster mode only
os.environ['PYSPARK_PYTHON'] = "./environment/env/bin/python"
spark = SparkSession.builder.config(
    "spark.yarn.dist.archives", "env.tar.gz#environment"
).appName("sPCA").getOrCreate()
```

### 6. Subir los archivos al HDFS
Subir los archivos de entrada al sistema HDFS de Hadoop y crea las carpetas necesarias:

```bash
hdfs dfs -put ./input/* /grupoh/challenge/input
hdfs dfs -mkdir /grupoh/challenge/output
```

### 7. Rezar ğŸ™

### 8. Ejecutar el cÃ³digo en el cluster
Ejecutar el cÃ³digo en el cluster de Spark con el siguiente comando:

```bash
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --archives env.tar.gz#environment \
  --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./environment/env/bin/python \
  --conf spark.executorEnv.PYSPARK_PYTHON=./environment/env/bin/python \
  run_spca.py \
  --input "hdfs:///grupoh/challenge/input/datos_1.txt" \
  --dim 2 \
  --maxIters 100 \
  --output "hdfs:///grupoh/challenge/output/datos_1_spark_pca"
```

### 9. VerificaciÃ³n de salida
Verificar que los archivos de entrada y salida se hayan cargado correctamente en las carpetas del HDFS. Al finalizar la ejecuciÃ³n, se  pueden encontrar los siguientes archivos en el directorio de salida:

```
/output
â”‚
â”œâ”€â”€ /datos_1_pca_spark_reduced  # Los datos con la dimensionalidad reducida
â”œâ”€â”€ /datos_1_pca_spark_C        # Componentes principales (la matriz C)
```
