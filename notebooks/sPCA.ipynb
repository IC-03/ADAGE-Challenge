{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb254e08",
   "metadata": {},
   "source": [
    "# Implementación de sPCA con pyspark!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c38c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b5281a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/12 13:28:50 WARN Utils: Your hostname, killua resolves to a loopback address: 127.0.1.1; using 10.14.35.163 instead (on interface wlo1)\n",
      "25/05/12 13:28:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/12 13:28:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# inicializar spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"sPCA\").getOrCreate()\n",
    "sc = spark.sparkContext  # así obtienes el SparkContext moderno\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e124c524",
   "metadata": {},
   "source": [
    "Algorithm 4 sPCA (Matrix Y , int N, int D, int d)\n",
    "1: C = normrnd(D, d)\n",
    "2: ss = normrnd(1, 1)\n",
    "3: Ym = meanJob(Y) # media de cada columna de Y\n",
    "4: ss1 = FnormJob(Y, Ym) # forbenius de la matriz de entrada\n",
    "5: while not STOP_CONDITION do\n",
    "\t6: M = C′ ∗C + ss ∗ I\n",
    "\t7: CM = C ∗ M−1\n",
    "\t8: Xm = Y m ∗CM\n",
    "\t9: {XtX,YtX} = YtXJob(Y,Y m, Xm,CM) # distribuido\n",
    "\t10: XtX+ = ss ∗ M−1\n",
    "\t11: C = YtX/XtX\n",
    "\t12: ss2 = trace(XtX ∗C′ ∗C)\n",
    "\t13: ss3 = ss3Job(Y,Y m, Xm,CM,C) # distribuido\n",
    "\t14: ss = (ss1 + ss2 − 2 ∗ ss3)/N/D\n",
    "15: end while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35e2fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la media de cada columna\n",
    "def meanJob(Y):\n",
    "    suma_vectores = Y.reduce(lambda a, b: [x + y for x, y in zip(a, b)])\n",
    "    n = Y.count()\n",
    "    media = [x / n for x in suma_vectores]\n",
    "    return media\n",
    "\n",
    "def center_data(Y, Ym):\n",
    "    return Y.map(lambda fila: [x - m for x, m in zip(fila, Ym)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6af69f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def FJob(Y_rdd, Ym):\n",
    "    diff_squared = Y_rdd.map(lambda row: sum((xi - mi) ** 2 for xi, mi in zip(row, Ym)))\n",
    "    fro_squared_sum = diff_squared.reduce(lambda a, b: a + b)\n",
    "    return np.sqrt(fro_squared_sum)\n",
    "\n",
    "\n",
    "# calcular norma de forbenius \n",
    "def frobenius_optimizado(Y, Ym):\n",
    "    msum = 0\n",
    "    sum = 0\n",
    "    for i in range(len(Ym)):\n",
    "        msum += (Ym[i] ** 2)\n",
    "    for i in range(len(Y)):\n",
    "        for j in range(len(Y[i])):\n",
    "            sum += (Y[i][j] - Ym[j]) ** 2\n",
    "            sum -= Ym[j]**2\n",
    "        sum += msum\n",
    "    return sum\n",
    "\n",
    "# Norma de Frobenius optimizada\n",
    "def FJob_no_sirve(Y, Ym):\n",
    "    # Calcular msum: la suma de los cuadrados de las medias\n",
    "    msum = sum([ym**2 for ym in Ym])\n",
    "    \n",
    "    # Calcular la suma de las diferencias cuadradas (Y_ij - Ym_j)^2\n",
    "    \n",
    "    sum_squared_diffs = Y.map(lambda row: sum([(row[j] - Ym[j])**2 for j in range(len(row))])).sum()\n",
    "\n",
    "    # Restar la suma de los cuadrados de las medias de cada columna (Ym_j^2)\n",
    "    sum_squared_diffs -= sum([ym**2 for ym in Ym])  # Se hace una corrección por los Ym^2\n",
    "\n",
    "    # Añadir msum, que es la suma de Ym_j^2\n",
    "    sum_squared_diffs += msum\n",
    "    \n",
    "    return sum_squared_diffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4cd4f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "#Clase acumuladora (usando listas de listas)\n",
    "class MatrixAccumulatorParam(AccumulatorParam):\n",
    "    def zero(self, value):\n",
    "        rows = len(value)\n",
    "        cols = len(value[0])\n",
    "        return [[0.0 for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "    def addInPlace(self, val1, val2):\n",
    "        for i in range(len(val1)):\n",
    "            for j in range(len(val1[0])):\n",
    "                val1[i][j] += val2[i][j]\n",
    "        return val1\n",
    "    \n",
    "# Wrapper de la clase acumuladora\n",
    "class MatrixAccumulatorWrapper:\n",
    "    def __init__(self, sc, shape):\n",
    "        self.shape = shape\n",
    "        self.accumulator = sc.accumulator(\n",
    "            [[0.0 for _ in range(shape[1])] for _ in range(shape[0])],\n",
    "            MatrixAccumulatorParam()\n",
    "        )\n",
    "\n",
    "    def addRow(self, row_index, row_contribution):\n",
    "        temp = [[0.0 for _ in range(self.shape[1])] for _ in range(self.shape[0])]\n",
    "        temp[row_index] = list(row_contribution)\n",
    "        self.accumulator.add(temp) \n",
    "        \n",
    "    def add(self, matrix):\n",
    "        self.accumulator.add(matrix)\n",
    "\n",
    "    def get(self):\n",
    "        return self.accumulator.value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9812642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def YtXSparkJob(Y, Ym, Xm, CM, D, d):\n",
    "    YtXSum = MatrixAccumulatorWrapper(sc, shape=(D, d))\n",
    "    XtXSum = MatrixAccumulatorWrapper(sc, shape=(d, d))\n",
    "    Ym = np.array(Ym)\n",
    "    Xm = np.array(Xm)\n",
    "    CM = np.array(CM)\n",
    "\n",
    "    # Broadcast variables to use inside workers\n",
    "    Ym_b = sc.broadcast(Ym)\n",
    "    Xm_b = sc.broadcast(Xm)\n",
    "    CM_b = sc.broadcast(CM)\n",
    "\n",
    "    def compute_partial(Yi):\n",
    "        try:\n",
    "            Yi = np.array(Yi)\n",
    "\n",
    "            CM = CM_b.value\n",
    "            Ym = Ym_b.value\n",
    "            Xm = Xm_b.value\n",
    "\n",
    "            Xi = (Yi @ CM) - (Ym @ CM)\n",
    "\n",
    "            YtX_i = np.outer(Yi - Ym, Xi - Xm)\n",
    "#            XtX_i = np.outer(Xi - Xm, Xi - Xm)\n",
    "            XtX_i = np.outer(Xi, Xi)\n",
    "\n",
    "            #YtX_i = np.outer(Yi, Xi_minus_Xm) - np.outer(Ym, Xi_minus_Xm)\n",
    "            #XtX_i = np.outer(Xi, Xi_minus_Xm) - np.outer(Xm, Xi_minus_Xm)\n",
    "            #non_zero_indices = np.nonzero(Yi)[0]\n",
    "            #for idx in non_zero_indices:\n",
    "            #    YtXSum.addRow(idx, YtX_i[idx, :])\n",
    "            YtXSum.add(YtX_i)\n",
    "            XtXSum.add(XtX_i)\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(\"Error in compute_partial:\")\n",
    "            traceback.print_exc()\n",
    "            raise e\n",
    "\n",
    "\n",
    "    Y.foreach(compute_partial)\n",
    "    YtX = YtXSum.accumulator.value\n",
    "    XtX = XtXSum.accumulator.value\n",
    "    return YtX, XtX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6b9e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ss3Job(Y, Ym, Xm, CM, C):\n",
    "    ss3 = sc.accumulator(0)\n",
    "    Ym = np.array(Ym)\n",
    "    Xm = np.array(Xm)\n",
    "    CM = np.array(CM)\n",
    "    C = np.array(C)\n",
    "\n",
    "    # Broadcast variables to use inside workers\n",
    "    Ym_b = sc.broadcast(Ym)\n",
    "    Xm_b = sc.broadcast(Xm)\n",
    "    CM_b = sc.broadcast(CM)\n",
    "    C_b = sc.broadcast(C)\n",
    "\n",
    "    def calculate_ss3(Yi):\n",
    "        try: \n",
    "            Yi = np.array(Yi)\n",
    "            CM = CM_b.value\n",
    "            Ym = Ym_b.value\n",
    "            Xm = Xm_b.value\n",
    "            C = C_b.value\n",
    "        \n",
    "            Xi = Yi @ CM - Ym @ CM\n",
    "            #Xi = Xi - Xm\n",
    "            cy = C.T @ Yi.T\n",
    "            xcy = Xi @ cy\n",
    "            ss3.add(xcy)\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(\"Error in calculate_ss3:\")\n",
    "            traceback.print_exc()\n",
    "            raise e\n",
    "    Y.foreach(calculate_ss3)\n",
    "    return ss3.value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc2395c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/12 13:29:05 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Ahora todo varias veces\n",
    "def has_converged(C_old, C_new, ss_old, ss_new, tol=1e-4):\n",
    "    delta_C = np.linalg.norm(C_new - C_old, ord='fro')\n",
    "    delta_ss = abs(ss_new - ss_old)\n",
    "    return delta_C < tol and delta_ss < tol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec9eda27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "# Leer archivo TXT como RDD\n",
    "rdd = sc.textFile(\"./input/datos_1.txt\")  # carga como RDD de líneas\n",
    "Y = rdd.map(lambda line: [float(x) for x in line.split(\",\")])\n",
    "D = 3  # number of columns\n",
    "d = 2  # dimension of the projection\n",
    "N = 17\n",
    "C = np.random.normal(loc=0.0, scale=1.0, size=(D, d))\n",
    "ss = np.random.normal(loc=0.0, scale=1.0)\n",
    "Ym = np.array(meanJob(Y))\n",
    "ss1 = FJob(Y, Ym)\n",
    "I = np.eye(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81e18618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ss : 0.2317643319123841\n",
      "2 ss : 0.009315808135117915\n",
      "3 ss : 0.00036567505548599485\n",
      "4 ss : 1.434073535854034e-05\n",
      "5 ss : 5.623826053561417e-07\n"
     ]
    }
   ],
   "source": [
    "max_iters = 10\n",
    "C_old =  np.inf\n",
    "ss_old = np.inf\n",
    "num_iters = 0\n",
    "while not has_converged(C_old, C, ss_old, ss) and num_iters < max_iters:\n",
    "    # Compute M = Cᵀ * C + ss * I\n",
    "    M = C.T @ C + ss * I\n",
    "\n",
    "    # Compute CM = C * M⁻¹\n",
    "    M_inv = np.linalg.inv(M)\n",
    "    CM = C @ M_inv\n",
    "\n",
    "    # Compute Xm = Ym * CM\n",
    "    Xm = Ym @ CM  # result has shape (d,)\n",
    "    YtX, XtX = YtXSparkJob(Y, Ym, Xm, CM, D, d)\n",
    "    XtX += ss * M_inv\n",
    "    XtX_inv = np.linalg.inv(XtX)\n",
    "    C_old = C\n",
    "    C = YtX @ XtX_inv\n",
    "    ss2 = np.trace(XtX @ C.T @ C)\n",
    "    ss3 = ss3Job(Y, Ym, Xm, CM, C)\n",
    "    ss_old = ss\n",
    "    ss = (ss1**2 + ss2 -2 * ss3) / N / D\n",
    "    num_iters +=1\n",
    "    print(f\"{num_iters} ss : {ss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb92bf",
   "metadata": {},
   "source": [
    "## Non distributed implementation of PPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "22cdd902",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "# Leer archivo TXT como RDD\n",
    "pathcsv = \"./input/datos_1.txt\"\n",
    "Y_p = np.genfromtxt(pathcsv, delimiter=\",\")  # convierte a numpy array\n",
    "D = Y_p.shape[1]  # number of columns\n",
    "d = 2  # dimension of the projection\n",
    "N = Y_p.shape[0]\n",
    "C_p = np.random.normal(loc=0.0, scale=1.0, size=(D, d))\n",
    "ss_p = np.random.normal(loc=0.0, scale=1.0)\n",
    "I = np.eye(d)\n",
    "C_old =  np.inf\n",
    "ss_old = np.inf\n",
    "Ym = np.sum(Y_p, axis=0) / N\n",
    "Yc = Y_p - Ym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "51055c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ss : 0.23176433191238355\n",
      "2 ss : 0.009315808135117358\n",
      "3 ss : 0.00036567505548599485\n",
      "4 ss : 1.434073535854034e-05\n",
      "5 ss : 5.623826042415649e-07\n"
     ]
    }
   ],
   "source": [
    "max_iters = 10\n",
    "num_iters = 0\n",
    "while not has_converged(C_old, C_p, ss_old, ss_p) and num_iters < max_iters:\n",
    "    M = C_p.T @ C_p + ss_p * I\n",
    "    M_inv = np.linalg.inv(M)\n",
    "    X_p = Yc @ C_p @ M_inv\n",
    "    XtX_p = X_p.T @ X_p + (ss_p * M_inv)\n",
    "    YtX_p = Yc.T @ X_p\n",
    "    C_old = C_p\n",
    "    C_p = YtX_p @ np.linalg.inv(XtX_p)\n",
    "    ss2_p = np.trace(XtX_p @ C_p.T @ C_p)\n",
    "    ss3_p = 0\n",
    "    for i in range(0,N):\n",
    "        ss3_p += X_p[i,:] @ C_p.T @ Yc[i,:].T\n",
    "    ss_old = ss_p\n",
    "    ss_p = (np.linalg.norm(Yc, ord='fro')**2 + ss2_p - 2 * ss3_p) / N / D\n",
    "    num_iters += 1\n",
    "    print(f\"{num_iters} ss : {ss_p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3fc989e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: \n",
      " [[ 2.97128795 -1.06407033]\n",
      " [ 0.89705396  2.29641661]\n",
      " [ 0.          0.        ]]\n",
      "ss_p:\n",
      " 5.623826042415649e-07\n"
     ]
    }
   ],
   "source": [
    "print(\"C: \\n\", C_p)\n",
    "print(\"ss_p:\\n\", ss_p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
