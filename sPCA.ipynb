{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb254e08",
   "metadata": {},
   "source": [
    "# Implementación de sPCA con pyspark!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c38c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b5281a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/08 09:04:52 WARN Utils: Your hostname, killua resolves to a loopback address: 127.0.1.1; using 10.14.13.175 instead (on interface wlo1)\n",
      "25/05/08 09:04:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/08 09:04:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/08 09:05:05 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# inicializar spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"sPCA\").getOrCreate()\n",
    "sc = spark.sparkContext  # así obtienes el SparkContext moderno\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e124c524",
   "metadata": {},
   "source": [
    "Algorithm 4 sPCA (Matrix Y , int N, int D, int d)\n",
    "1: C = normrnd(D, d)\n",
    "2: ss = normrnd(1, 1)\n",
    "3: Ym = meanJob(Y) # media de cada columna de Y\n",
    "4: ss1 = FnormJob(Y, Ym) # forbenius de la matriz de entrada\n",
    "5: while not STOP_CONDITION do\n",
    "\t6: M = C′ ∗C + ss ∗ I\n",
    "\t7: CM = C ∗ M−1\n",
    "\t8: Xm = Y m ∗CM\n",
    "\t9: {XtX,YtX} = YtXJob(Y,Y m, Xm,CM) # distribuido\n",
    "\t10: XtX+ = ss ∗ M−1\n",
    "\t11: C = YtX/XtX\n",
    "\t12: ss2 = trace(XtX ∗C′ ∗C)\n",
    "\t13: ss3 = ss3Job(Y,Y m, Xm,CM,C) # distribuido\n",
    "\t14: ss = (ss1 + ss2 − 2 ∗ ss3)/N/D\n",
    "15: end while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e2fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la media de cada columna\n",
    "def meanJob(Y):\n",
    "    suma_vectores = Y.reduce(lambda a, b: [x + y for x, y in zip(a, b)])\n",
    "    n = Y.count()\n",
    "    media = [x / n for x in suma_vectores]\n",
    "    return media\n",
    "\n",
    "def center_data(Y, Ym):\n",
    "    return Y.map(lambda fila: [x - m for x, m in zip(fila, Ym)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "6af69f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def FJob(Y_rdd, Ym):\n",
    "    diff_squared = Y_rdd.map(lambda row: sum((xi - mi) ** 2 for xi, mi in zip(row, Ym)))\n",
    "    fro_squared_sum = diff_squared.reduce(lambda a, b: a + b)\n",
    "    return np.sqrt(fro_squared_sum)\n",
    "\n",
    "\n",
    "# calcular norma de forbenius \n",
    "def frobenius_optimizado(Y, Ym):\n",
    "    msum = 0\n",
    "    sum = 0\n",
    "    for i in range(len(Ym)):\n",
    "        msum += (Ym[i] ** 2)\n",
    "    for i in range(len(Y)):\n",
    "        for j in range(len(Y[i])):\n",
    "            sum += (Y[i][j] - Ym[j]) ** 2\n",
    "            sum -= Ym[j]**2\n",
    "        sum += msum\n",
    "    return sum\n",
    "\n",
    "# Norma de Frobenius optimizada\n",
    "def FJob_no_sirve(Y, Ym):\n",
    "    # Calcular msum: la suma de los cuadrados de las medias\n",
    "    msum = sum([ym**2 for ym in Ym])\n",
    "    \n",
    "    # Calcular la suma de las diferencias cuadradas (Y_ij - Ym_j)^2\n",
    "    \n",
    "    sum_squared_diffs = Y.map(lambda row: sum([(row[j] - Ym[j])**2 for j in range(len(row))])).sum()\n",
    "\n",
    "    # Restar la suma de los cuadrados de las medias de cada columna (Ym_j^2)\n",
    "    sum_squared_diffs -= sum([ym**2 for ym in Ym])  # Se hace una corrección por los Ym^2\n",
    "\n",
    "    # Añadir msum, que es la suma de Ym_j^2\n",
    "    sum_squared_diffs += msum\n",
    "    \n",
    "    return sum_squared_diffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f4cd4f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "#Clase acumuladora (usando listas de listas)\n",
    "class MatrixAccumulatorParam(AccumulatorParam):\n",
    "    def zero(self, value):\n",
    "        rows = len(value)\n",
    "        cols = len(value[0])\n",
    "        return [[0.0 for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "    def addInPlace(self, val1, val2):\n",
    "        for i in range(len(val1)):\n",
    "            for j in range(len(val1[0])):\n",
    "                val1[i][j] += val2[i][j]\n",
    "        return val1\n",
    "    \n",
    "# Wrapper de la clase acumuladora\n",
    "class MatrixAccumulatorWrapper:\n",
    "    def __init__(self, sc, shape):\n",
    "        self.shape = shape\n",
    "        self.accumulator = sc.accumulator(\n",
    "            [[0.0 for _ in range(shape[1])] for _ in range(shape[0])],\n",
    "            MatrixAccumulatorParam()\n",
    "        )\n",
    "\n",
    "    def addRow(self, row_index, row_contribution):\n",
    "        temp = [[0.0 for _ in range(self.shape[1])] for _ in range(self.shape[0])]\n",
    "        temp[row_index] = list(row_contribution)\n",
    "        self.accumulator.add(temp) \n",
    "        \n",
    "    def add(self, matrix):\n",
    "        self.accumulator.add(matrix)\n",
    "\n",
    "    def get(self):\n",
    "        return self.accumulator.value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9812642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def YtXSparkJob(Y, Ym, Xm, CM, D, d):\n",
    "    YtXSum = MatrixAccumulatorWrapper(sc, shape=(D, d))\n",
    "    XtXSum = MatrixAccumulatorWrapper(sc, shape=(d, d))\n",
    "    Ym = np.array(Ym)\n",
    "    Xm = np.array(Xm)\n",
    "    CM = np.array(CM)\n",
    "\n",
    "    # Broadcast variables to use inside workers\n",
    "    Ym_b = sc.broadcast(Ym)\n",
    "    Xm_b = sc.broadcast(Xm)\n",
    "    CM_b = sc.broadcast(CM)\n",
    "    YtXSum_b = YtXSum  # since it's a custom wrapper around an accumulator\n",
    "    XtXSum_b = XtXSum\n",
    "    def compute_partial(Yi):\n",
    "        try:\n",
    "            Yi = np.array(Yi)\n",
    "\n",
    "            CM = CM_b.value\n",
    "            Ym = Ym_b.value\n",
    "            Xm = Xm_b.value\n",
    "\n",
    "            Xi = Yi @ CM - Ym @ CM\n",
    "            Xi_minus_Xm = Xi - Xm\n",
    "\n",
    "            YtX_i = np.outer(Yi.T, Xi_minus_Xm) - np.outer(Ym.T, Xi_minus_Xm)\n",
    "            XtX_i = np.outer(Xi.T, Xi_minus_Xm) - np.outer(Xm.T, Xi_minus_Xm)\n",
    "\n",
    "            non_zero_indices = np.nonzero(Yi)[0]\n",
    "            for idx in non_zero_indices:\n",
    "                YtXSum_b.addRow(idx, YtX_i[idx, :])\n",
    "\n",
    "            XtXSum_b.add(XtX_i)\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(\"Error in compute_partial:\")\n",
    "            traceback.print_exc()\n",
    "            raise e\n",
    "\n",
    "\n",
    "    Y.foreach(compute_partial)\n",
    "    YtX = YtXSum.accumulator.value\n",
    "    XtX = XtXSum.accumulator.value\n",
    "    return YtX, XtX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "e6b9e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ss3Job(Y, Ym, Xm, CM, C):\n",
    "    ss3 = sc.accumulator(0)\n",
    "    Ym = np.array(Ym)\n",
    "    Xm = np.array(Xm)\n",
    "    CM = np.array(CM)\n",
    "    C = np.array(C)\n",
    "\n",
    "    # Broadcast variables to use inside workers\n",
    "    Ym_b = sc.broadcast(Ym)\n",
    "    Xm_b = sc.broadcast(Xm)\n",
    "    CM_b = sc.broadcast(CM)\n",
    "    C_b = sc.broadcast(C)\n",
    "\n",
    "    def calculate_ss3(Yi):\n",
    "        try: \n",
    "            Yi = np.array(Yi)\n",
    "            CM = CM_b.value\n",
    "            Ym = Ym_b.value\n",
    "            Xm = Xm_b.value\n",
    "            C = C_b.value\n",
    "        \n",
    "            Xi = Yi @ CM - Ym @ CM\n",
    "            Xi = Xi - Xm\n",
    "            cy = C.T @ Yi.T\n",
    "            xcy = Xi @ cy\n",
    "            ss3.add(xcy)\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(\"Error in calculate_ss3:\")\n",
    "            traceback.print_exc()\n",
    "            raise e\n",
    "    Y.foreach(calculate_ss3)\n",
    "    return ss3.value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9eda27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer archivo TXT como RDD\n",
    "rdd = sc.textFile(\"./input/datos_1.txt\")  # carga como RDD de líneas\n",
    "Y = rdd.map(lambda line: [float(x) for x in line.split(\",\")])\n",
    "D = 3  # number of columns\n",
    "d = 2  # dimension of the projection\n",
    "N = 17\n",
    "C = np.random.normal(loc=0.0, scale=1.0, size=(D, d))\n",
    "ss = np.random.normal(loc=0.0, scale=1.0)\n",
    "Ym = np.array(meanJob(Y))\n",
    "ss1 = FJob(Y, Ym)\n",
    "I = np.eye(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "4f3be0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute M = Cᵀ * C + ss * I\n",
    "M = C.T @ C + ss * I\n",
    "\n",
    "# Compute CM = C * M⁻¹\n",
    "M_inv = np.linalg.inv(M)\n",
    "CM = C @ M_inv\n",
    "\n",
    "# Compute Xm = Ym * CM\n",
    "Xm = Ym @ CM  # result has shape (d,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "575851f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "YtX, XtX = YtXSparkJob(Y, Ym, Xm, CM, D, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "fe973237",
   "metadata": {},
   "outputs": [],
   "source": [
    "XtX += ss * M_inv\n",
    "XtX_inv = np.linalg.inv(XtX)\n",
    "C = YtX @ XtX_inv\n",
    "ss2 = np.trace(XtX @ C.T @ C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "aadd1101",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss3 = ss3Job(Y, Ym, Xm, CM, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "008a99a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = (ss1 + ss2 -2 * ss3) / N / D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "fc2395c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora todo varias veces\n",
    "def has_converged(C_old, C_new, ss_old, ss_new, tol=1e-4):\n",
    "    delta_C = np.linalg.norm(C_new - C_old, ord='fro')\n",
    "    delta_ss = abs(ss_new - ss_old)\n",
    "    return delta_C < tol and delta_ss < tol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "315f8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "# Leer archivo TXT como RDD\n",
    "rdd = sc.textFile(\"./input/datos_1.txt\")  # carga como RDD de líneas\n",
    "Y = rdd.map(lambda line: [float(x) for x in line.split(\",\")])\n",
    "D = 3  # number of columns\n",
    "d = 2  # dimension of the projection\n",
    "N = Y.count()\n",
    "C = np.random.normal(loc=0.0, scale=1.0, size=(D, d))\n",
    "ss = abs(np.random.normal(loc=1.0, scale=0.1))\n",
    "Ym = np.array(meanJob(Y))\n",
    "ss1 = FJob(Y, Ym)\n",
    "I = np.eye(d)\n",
    "\n",
    "max_iters = 1\n",
    "C_old =  np.inf\n",
    "ss_old = np.inf\n",
    "num_iters = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "e129aab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49671415, -0.1382643 ],\n",
       "       [ 0.64768854,  1.52302986],\n",
       "       [-0.23415337, -0.23413696]])"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "81e18618",
   "metadata": {},
   "outputs": [],
   "source": [
    "while not has_converged(C_old, C, ss_old, ss) and num_iters == max_iters:\n",
    "    # Compute M = Cᵀ * C + ss * I\n",
    "    M = C.T @ C + ss * I\n",
    "\n",
    "    # Compute CM = C * M⁻¹\n",
    "    M_inv = np.linalg.inv(M)\n",
    "    CM = C @ M_inv\n",
    "\n",
    "    # Compute Xm = Ym * CM\n",
    "    Xm = Ym @ CM  # result has shape (d,)\n",
    "    YtX, XtX = YtXSparkJob(Y, Ym, Xm, CM, D, d)\n",
    "    XtX += ss * M_inv\n",
    "    XtX_inv = np.linalg.inv(XtX)\n",
    "    C_old = C\n",
    "    C = YtX @ XtX_inv\n",
    "    ss2 = np.trace(XtX @ C.T @ C)\n",
    "    ss3 = ss3Job(Y, Ym, Xm, CM, C)\n",
    "    ss_old = ss\n",
    "    ss = (ss1 + ss2 -2 * ss3) / N / D\n",
    "    num_iters +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "988e681e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M:\n",
      "  [[1.87897448 0.9725951 ]\n",
      " [0.9725951  3.55147836]]\n",
      "Xm \n",
      " [ 0.35867453 -1.30560805]\n",
      "XtX\n",
      " [[  5.57201365 -15.32459479]\n",
      " [-15.32459479  52.89378186]]\n",
      "YtX\n",
      " [[np.float64(6.198586744913513), np.float64(-18.88353346895369)], [np.float64(-3.0143604890505844), np.float64(9.305774505185928)], [np.float64(0.0), np.float64(0.0)]]\n",
      "C\n",
      " [[ 0.49671415 -0.1382643 ]\n",
      " [ 0.64768854  1.52302986]\n",
      " [-0.23415337 -0.23413696]]\n",
      "ss2\n",
      " 8.93583614178467\n",
      "ss3\n",
      " -1.0192034984372622\n",
      "ss\n",
      " 1.1579212815507391\n"
     ]
    }
   ],
   "source": [
    "# Compute M = Cᵀ * C + ss * I\n",
    "print(\"M:\\n \", M)\n",
    "print(\"Xm \\n\", Xm)\n",
    "print(\"XtX\\n\", XtX)\n",
    "print(\"YtX\\n\", YtX)\n",
    "print(\"C\\n\", C)\n",
    "print(\"ss2\\n\", ss2)\n",
    "print(\"ss3\\n\", ss3)\n",
    "print(\"ss\\n\", ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab07da01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"M: \\n\", M)\n",
    "print(\"X \\n\",X_p)\n",
    "print(\"XtX\\n\", XtX_p)\n",
    "print(\"YtX\\n\", YtX_p)\n",
    "print(\"C\\n\", C_p)\n",
    "print(\"ss2\\n\", ss2_p)\n",
    "print(\"ss3\\n\", ss3_p)\n",
    "print(\"ss\\n\", ss_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "22cdd902",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "# Leer archivo TXT como RDD\n",
    "rdd = sc.textFile(\"./input/datos_1.txt\")  # carga como RDD de líneas\n",
    "Y = rdd.map(lambda line: [float(x) for x in line.split(\",\")])\n",
    "D = 3  # number of columns\n",
    "d = 2  # dimension of the projection\n",
    "N = Y.count()\n",
    "C_p = np.random.normal(loc=0.0, scale=1.0, size=(D, d))\n",
    "ss_p = abs(np.random.normal(loc=1.0, scale=0.1))\n",
    "I = np.eye(d)\n",
    "C_old =  np.inf\n",
    "ss_old = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "51055c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "pathcsv = \"./input/datos_1.txt\"\n",
    "Y_p = np.genfromtxt(pathcsv, delimiter=\",\")  # convierte a numpy array\n",
    "\n",
    "D = 3  # number of columns\n",
    "d = 2  # dimension of the projection\n",
    "N = 17\n",
    "C_p = np.random.normal(loc=0.0, scale=1.0, size=(D, d))\n",
    "ss_p = abs(np.random.normal(loc=1.0, scale=0.1))\n",
    "I = np.eye(d)\n",
    "C_old =  np.inf\n",
    "ss_old = np.inf\n",
    "Ym = np.sum(Y_p, axis=0) / N\n",
    "Yc = Y_p - Ym\n",
    "max_iters = 1000\n",
    "num_iters = 0\n",
    "while num_iters < max_iters:\n",
    "    M = C_p.T @ C_p + ss_p * I\n",
    "    M_inv = np.linalg.inv(M)\n",
    "    X_p = Yc @ C_p @ M_inv\n",
    "    XtX_p = X_p.T @ X_p + ss_p * M_inv\n",
    "    YtX_p = Yc.T @ X_p\n",
    "    C_old = C_p\n",
    "    C_p = YtX_p @ np.linalg.inv(XtX_p)\n",
    "    ss2_p = np.trace(XtX_p @ C_p.T @ C_p)\n",
    "    ss3_p = np.trace(X_p @ C_p.T @ Yc.T)\n",
    "    ss_old = ss_p\n",
    "    ss_p = (np.linalg.norm(Yc, ord='fro')**2 + ss2_p - 2 * ss3_p) / (N * D)\n",
    "    num_iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "123a45f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.45931816, -0.86903657],\n",
       "       [ 0.84413307,  2.13054494],\n",
       "       [ 0.        ,  0.        ]])"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
